介绍：该代码是python语言编写的，为问题一的处理代码。
import os
import numpy as np
import pandas as pd
import scipy.io
import scipy.signal
from scipy.fftpack import fft
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# 参数配置
data_dir = 'C:/Users/zpq/Desktop/E题/源域数据集/'
selected_position = 'DE'
fault_dict = {'N': 0, 'B': 1, 'IR': 2, 'OR': 3}

def get_all_mat_files(data_dir):
    mat_files = []
    for root, dirs, files in os.walk(data_dir):
        for f in files:
            if f.endswith('.mat'):
                mat_files.append(os.path.join(root, f))
    return mat_files

def load_data_label(file_path, selected_position):
    data = scipy.io.loadmat(file_path)
    signal_key = None
    for var_name in data.keys():
        if selected_position in var_name and 'time' in var_name:
            signal_key = var_name
            break
    if signal_key is None:
        raise ValueError(f"文件{file_path}未找到{selected_position}部位信号（需符合Xxxx_{selected_position}_time格式）")
    signal = np.array(data[signal_key]).flatten()
    file_name = os.path.basename(file_path).split('.')[0]

    # 解析故障类型与标签
    if file_name.startswith('IR'):
        fault_type = 'IR'
        label = 2
    elif file_name.startswith('OR'):
        fault_type = 'OR'
        label = 3
    elif file_name.startswith(('B', 'N')):
        fault_type = file_name[0]
        label = fault_dict.get(fault_type, -1)
    else:
        fault_type = 'Unknown'
        label = -1  # 未识别故障类型

    # 解析故障尺寸
    fault_size = np.nan
    if fault_type in ('IR', 'OR'):
        remaining = file_name[2:]
        size_str = ''.join([c for c in remaining if c.isdigit()][:3])
        if len(size_str) == 3:
            fault_size = float(size_str) / 1000
    elif fault_type in ('B', 'N') and len(file_name) >= 4:
        size_str = file_name[1:4]
        if size_str.isdigit():
            fault_size = float(size_str) / 1000

    # 解析载荷
    load = np.nan
    if '_' in file_name:
        last_under_idx = file_name.rfind('_')
        load_str = ''.join([c for c in file_name[last_under_idx + 1:] if c.isdigit()])
        if load_str:
            load = int(load_str)
    return signal, label, file_name, fault_size, load

# 提取采样率
def get_sample_rate_from_path(file_path):
    if '12kHz' in file_path:
        return 12000
    elif '48kHz' in file_path:
        return 48000
    else:
        raise ValueError(f"路径{file_path}未识别采样率！")

# 提取时域+频域特征
def extract_features(signal, sample_rate):
    # 时域特征（9个）
    mean_val = np.mean(signal)
    std_val = np.std(signal)
    skew_val = stats.skew(signal)
    kurt_val = stats.kurtosis(signal)
    rms_val = np.sqrt(np.mean(np.square(signal)))
    peak_val = np.max(np.abs(signal))
    crest_factor = peak_val / rms_val if rms_val != 0 else 0
    impulse_factor = peak_val / np.mean(np.abs(signal)) if np.mean(np.abs(signal)) != 0 else 0
    shape_factor = rms_val / np.mean(np.abs(signal)) if np.mean(np.abs(signal)) != 0 else 0

    # 频域特征（5个）
    n = len(signal)
    fft_complex = fft(signal)
    fft_mag = np.abs(fft_complex)
    freq_axis = np.fft.fftfreq(n, d=1 / sample_rate)
    positive_mask = freq_axis > 0
    freq_pos = freq_axis[positive_mask]
    fft_mag_pos = np.maximum(fft_mag[positive_mask], 1e-10)

    freq_centroid = np.sum(freq_pos * fft_mag_pos) / np.sum(fft_mag_pos)
    fft_prob = fft_mag_pos / np.sum(fft_mag_pos)
    spec_entropy = -np.sum(fft_prob * np.log(fft_prob))
    top3_peaks = np.argsort(fft_mag_pos)[-3:][::-1]
    freq_peak1, freq_peak2, freq_peak3 = freq_pos[top3_peaks]

    # 整合
    features = [
        mean_val, std_val, skew_val, kurt_val, rms_val, peak_val,
        crest_factor, impulse_factor, shape_factor,
        freq_centroid, spec_entropy, freq_peak1, freq_peak2, freq_peak3
    ]
    feature_names = [
        'mean', 'std', 'skew', 'kurtosis', 'rms', 'peak',
        'crest_factor', 'impulse_factor', 'shape_factor',
        'freq_centroid', 'spec_entropy', 'freq_peak1', 'freq_peak2', 'freq_peak3'
    ]
    return features, feature_names

#可视化
def visualize_data(csv_path):
    df = pd.read_csv(csv_path)
    print(f"\n开始可视化分析（基于{len(df)}个样本）")
    plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'SimHei', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False

    # 图表1：故障类型分布
    print("正在绘制图表1：故障类型分布...")
    plt.figure(figsize=(8, 4))
    sns.countplot(x='fault_type', hue='fault_type', data=df, order=['N', 'B', 'IR', 'OR'],
                  palette='Set2', legend=False)
    plt.title('故障类型样本分布', fontsize=12)
    plt.xlabel('故障类型（N=正常，B=滚动体，IR=内圈，OR=外圈）', fontsize=10)
    plt.ylabel('样本数量', fontsize=10)
    for p in plt.gca().patches:
        plt.gca().annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2, p.get_height()),
                           ha='center', va='center', xytext=(0, 5), textcoords='offset points')
    plt.tight_layout()
    plt.savefig('1_故障类型分布.png', dpi=300)
    plt.show(block=False)
    print("图表1绘制并保存完成")

    # 图表2：故障尺寸分布
    df_size = df.dropna(subset=['fault_size_inch'])
    if len(df_size) > 0:
        print("正在绘制图表2：不同故障类型的故障尺寸分布...")
        plt.figure(figsize=(10, 6))
        sns.countplot(x='fault_size_inch', hue='fault_type', data=df_size, palette='Set3')
        plt.title('不同故障类型的故障尺寸分布', fontsize=12)
        plt.xlabel('故障尺寸（英寸）', fontsize=10)
        plt.ylabel('样本数量', fontsize=10)
        plt.legend(title='故障类型', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig('2_故障尺寸分布.png', dpi=300)
        plt.show(block=False)
        print("图表2绘制并保存完成")
    else:
        print("!图表2：无有效故障尺寸数据，跳过绘制")

    # 图表3：峰度分布
    print("正在绘制图表3：不同故障类型的峰度分布...")
    plt.figure(figsize=(8, 4))
    sns.boxplot(
        x='fault_type',
        y='kurtosis',
        hue='fault_type',
        data=df,
        order=['N', 'B', 'IR', 'OR'],
        palette='Set2',
        legend=False
    )
    plt.title('不同故障类型的峰度分布', fontsize=12)
    plt.xlabel('故障类型', fontsize=10)
    plt.ylabel('峰度值（正常≈3，故障>3）', fontsize=10)
    plt.axhline(y=3, color='red', linestyle='--', alpha=0.7, label='正常参考线')
    plt.legend()
    plt.tight_layout()
    plt.savefig('3_峰度分布.png', dpi=300)
    plt.show(block=False)
    print("图表3绘制并保存完成")

    #  图表4：故障尺寸与RMS关系
    df_rms = df.dropna(subset=['fault_size_inch', 'rms'])
    if len(df_rms) > 0:
        print("正在绘制图表4：故障尺寸与RMS值的关系...")
        plt.figure(figsize=(8, 4))
        sns.lineplot(x='fault_size_inch', y='rms', hue='fault_type', data=df_rms,
                     marker='o', linewidth=2, markersize=6, palette='Set3')
        plt.title('故障尺寸与RMS值的关系', fontsize=12)
        plt.xlabel('故障尺寸（英寸）', fontsize=10)
        plt.ylabel('RMS值（振动能量）', fontsize=10)
        plt.legend(title='故障类型', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig('4_故障尺寸与RMS关系.png', dpi=300)
        plt.show(block=False)
        print("图表4绘制并保存完成")
    else:
        print("!图表4：无有效故障尺寸/RMS数据，跳过绘制")

    # 图表5：不同采样率的频率重心对比
    df_de = df[df['position'] == 'DE']
    if df_de['sample_rate_hz'].nunique() >= 2:
        print("正在绘制图表5：DE端不同采样率的频率重心分布...")
        plt.figure(figsize=(8, 4))
        df_de['sample_rate_str'] = df_de['sample_rate_hz'].astype(str) + 'Hz'
        sns.boxplot(x='fault_type', y='freq_centroid', hue='sample_rate_str',
                    data=df_de, order=['N', 'B', 'IR', 'OR'], palette='Set2')
        plt.title('DE端不同采样率的频率重心分布', fontsize=12)
        plt.xlabel('故障类型', fontsize=10)
        plt.ylabel('频率重心（Hz）', fontsize=10)
        plt.legend(title='采样率', bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.tight_layout()
        plt.savefig('5_频率重心与采样率关系.png', dpi=300)
        plt.show(block=False)
        print("图表5绘制并保存完成")
    else:
        print("!图表5：DE端采样率类型不足2种，跳过绘制")
    plt.show()
    print("所有可视化图表绘制完成！")

# 主流程
if __name__ == "__main__":
    mat_files = get_all_mat_files(data_dir)
    if len(mat_files) == 0:
        print(f"警告：{data_dir}未找到任何.mat文件！请检查路径")
        exit()
    print(f"成功发现{len(mat_files)}个MAT文件，开始处理...")

    # 初始化
    features_list = []
    label_list = []
    file_name_list = []
    fault_size_list = []
    load_list = []
    sample_rate_list = []
    position_list = []

    # 循环处理每个文件
    for idx, file_path in enumerate(mat_files, 1):
        try:
            sample_rate = get_sample_rate_from_path(file_path)
            signal, label, file_name, fault_size, load = load_data_label(file_path, selected_position)
            if label == -1:
                print(f"跳过{idx}/{len(mat_files)}：{file_name}.mat（未识别故障类型）")
                continue
            target_len = sample_rate * 8  #
            if len(signal) > target_len:
                signal = signal[:target_len]
                print(f"截取{idx}/{len(mat_files)}：{file_name}.mat 前8秒信号")
            else:
                print(f"注意{idx}/{len(mat_files)}：{file_name}.mat 信号长度不足8秒（实际{len(signal)/sample_rate:.1f}秒），使用完整信号")

            features, feature_names = extract_features(signal, sample_rate)
            # 存储结果
            features_list.append(features)
            label_list.append(label)
            file_name_list.append(file_name)
            fault_size_list.append(fault_size)
            load_list.append(load)
            sample_rate_list.append(sample_rate)
            position_list.append(selected_position)
            # 打印进度
            print(f"完成{idx}/{len(mat_files)}：{file_name}.mat（标签：{label}，采样率：{sample_rate}Hz）")

        except Exception as e:
            print(f"失败{idx}/{len(mat_files)}：{file_path} | 错误：{str(e)}")
            continue

    # 检查有效数据
    if len(features_list) == 0:
        print("警告：未处理任何有效文件！请检查数据格式")
        exit()

    # 构建DataFrame并保存
    df_features = pd.DataFrame(features_list, columns=feature_names)
    df_meta = pd.DataFrame({
        'file_name': file_name_list,
        'position': position_list,
        'sample_rate_hz': sample_rate_list,
        'fault_type': [list(fault_dict.keys())[list(fault_dict.values()).index(l)] for l in label_list],
        'fault_type_cn': ['正常' if l == 0 else '滚动体' if l == 1 else '内圈' if l == 2 else '外圈' for l in
                          label_list],
        'label': label_list,
        'fault_size_inch': fault_size_list,
        'load_hp': load_list
    })
    df_final = pd.concat([df_meta, df_features], axis=1)
    csv_path = '轴承特征_完整数据.csv'
    df_final.to_csv(csv_path, index=False, encoding='utf-8-sig')
    print(f"\n所有数据已保存至：{csv_path}（有效样本数：{len(df_final)}）")

    df = pd.read_csv(csv_path)
    print("\n【数据分布验证】")
    # 1. 故障类型分布
    print("故障类型计数：")
    print(df['fault_type'].value_counts())
    # 2. 故障尺寸非空情况
    print("\n故障尺寸非空数量：")
    print(df['fault_size_inch'].count())
    # 3. 采样率类型
    print("\n采样率类型：")
    print(df['sample_rate_hz'].unique())
    # 4. RMS 非空情况
    print("\nRMS 非空数量：")
    print(df['rms'].count())

    print("开始执行可视化...")
    visualize_data(csv_path)
附录2
介绍：该代码是python语言编写的，为问题二的处理代码。
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import (
    classification_report, confusion_matrix,
    accuracy_score, roc_auc_score, roc_curve,
    r2_score
)
import xgboost as xgb
import warnings
warnings.filterwarnings('ignore')

# 数据增强
def augment_data(X, y, noise_ratio=0.01):
    np.random.seed(42)
    feature_std = np.std(X, axis=0, keepdims=True)
    noise = np.random.normal(0, feature_std * noise_ratio, X.shape)
    X_augmented = np.vstack([X, X + noise])
    y_augmented = np.hstack([y, y])
    print(f"数据增强后：样本量从{len(X)}增至{len(X_augmented)}")
    return X_augmented, y_augmented

# 特征筛选函数
def select_top_features(X, feature_cols, importances, top_k=10):
    idx_sorted = np.argsort(importances)[::-1][:top_k]
    X_selected = X[:, idx_sorted]
    selected_cols = [feature_cols[i] for i in idx_sorted]
    print(f"特征筛选：保留Top {top_k} 特征，从 {len(feature_cols)} 个特征中")
    print(f"   保留特征：{selected_cols}")
    return X_selected, selected_cols

# 数据加载与预处理
def load_and_preprocess_data(csv_path):
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    print(f"成功加载特征矩阵，共 {len(df)} 个样本，{len(df.columns)} 个字段")

    feature_cols = [
        'mean', 'std', 'skew', 'kurtosis', 'rms', 'peak',
        'crest_factor', 'impulse_factor', 'shape_factor',
        'freq_centroid', 'spec_entropy', 'freq_peak1', 'freq_peak2', 'freq_peak3'
    ]
    label_col = 'label'

    df_valid = df.dropna(subset=feature_cols + [label_col])
    print(f"过滤无效样本后，剩余 {len(df_valid)} 个有效样本")

    y_original = df_valid[label_col].values
    unique_labels = np.unique(y_original)
    print(f"原始标签值：{unique_labels}")

    label_mapping = {label: i for i, label in enumerate(sorted(unique_labels))}
    y_encoded = np.array([label_mapping[label] for label in y_original])
    num_classes = len(unique_labels)
    print(f"标签映射：{label_mapping}，共 {num_classes} 个类别")

    X = df_valid[feature_cols].values

    # 数据增强
    if len(X) < 200:
        X, y_encoded = augment_data(X, y_encoded)
    return X, y_encoded, feature_cols, label_mapping, num_classes, df_valid


# 数据集划分
def split_train_test(X, y, test_size=0.3, random_state=42):
    class0_idx = np.where(y == 0)[0]
    if len(class0_idx) >= 2:
        test_class0 = class0_idx[:2]
        rest_idx = [i for i in range(len(y)) if i not in test_class0]
        X_rest, y_rest = X[rest_idx], y[rest_idx]
        adjusted_test_size = (len(y) * test_size - 2) / len(y_rest) if len(y_rest) > 0 else test_size
        adjusted_test_size = max(0.1, min(0.5, adjusted_test_size))

        X_train_rest, X_test_rest, y_train_rest, y_test_rest = train_test_split(
            X_rest, y_rest, test_size=adjusted_test_size,
            stratify=y_rest, random_state=random_state
        )
        X_test = np.vstack([X[test_class0], X_test_rest]) if len(X_test_rest) > 0 else X[test_class0]
        y_test = np.hstack([y[test_class0], y_test_rest]) if len(y_test_rest) > 0 else y[test_class0]
        X_train = X_train_rest
        y_train = y_train_rest
    else:
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, stratify=y, random_state=random_state
        )
        print("类别0样本过少（<2个），测试集结果可能不可靠")

    print(f"\n数据集划分结果：")
    print(f"  - 训练集：{len(X_train)} 个样本（类别分布：{np.bincount(y_train)}）")
    print(f"  - 测试集：{len(X_test)} 个样本（类别分布：{np.bincount(y_test)}）")
    return X_train, X_test, y_train, y_test


# XGBoost模型构建与训练
def train_xgboost_classifier(X_train, y_train, num_classes, random_state=42):
    eval_set = [(X_train, y_train)]

    class_counts = np.bincount(y_train)
    class_counts = np.maximum(class_counts, 1)
    scale_pos_weight = [1,3,3,1]
    print(f" 类别权重：{scale_pos_weight}")

    xgb_clf = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=num_classes,
        n_estimators=80,
        max_depth=2,
        learning_rate=0.15,
        reg_alpha=0.3,
        reg_lambda=0.3,
        subsample=0.6,
        colsample_bytree=0.7,
        eval_metric='mlogloss',
        early_stopping_rounds=8,
        scale_pos_weight=scale_pos_weight,
        random_state=random_state,
        verbosity=0
    )

    print("\n 开始训练XGBoost模型...")
    xgb_clf.fit(
        X_train, y_train,
        eval_set=eval_set,
        verbose=False
    )

    best_iteration = xgb_clf.best_iteration if hasattr(xgb_clf, 'best_iteration') else xgb_clf.best_iteration_
    print(f" XGBoost模型训练完成（最佳迭代轮次：{best_iteration}）")

    y_train_pred = xgb_clf.predict(X_train)
    train_r2 = r2_score(y_train, y_train_pred)
    print(f"训练集R²值：{train_r2:.4f}")

    return xgb_clf

#  模型评估
def evaluate_model(xgb_clf, X_train, X_test, y_train, y_test, label_mapping, feature_cols, num_classes):
    y_pred = xgb_clf.predict(X_test)
    y_pred_proba = xgb_clf.predict_proba(X_test)

    print("\n源域故障诊断模型分类报告：")
    class_names = [f'类别{i} ({k})' for i, k in label_mapping.items()]
    print(classification_report(
        y_test, y_pred,
        target_names=class_names,
        digits=4
    ))

    test_r2 = r2_score(y_test, y_pred)
    print(f"模型测试集准确率：{accuracy_score(y_test, y_pred):.4f}")
    print(f"模型测试集R²值：{test_r2:.4f}")

    # 混淆矩阵
    plt.rcParams['font.sans-serif'] = ['SimHei', 'WenQuanYi Zen Hei', 'Microsoft YaHei']  # 优先使用SimHei
    plt.rcParams['axes.unicode_minus'] = False

    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('预测故障类型')
    plt.ylabel('真实故障类型')
    plt.title(f'混淆矩阵 (R$^2$={test_r2:.4f})')
    plt.tight_layout()
    plt.savefig('2_混淆矩阵.png', dpi=300)
    plt.show()
    print("混淆矩阵已保存")

    # ROC曲线
    plt.figure(figsize=(8, 6))
    for i, class_name in enumerate(class_names):
        y_test_binary = (y_test == i).astype(int)
        fpr, tpr, _ = roc_curve(y_test_binary, y_pred_proba[:, i])
        auc_score = roc_auc_score(y_test_binary, y_pred_proba[:, i])
        plt.plot(fpr, tpr, label=f'{class_name} (AUC={auc_score:.4f})')

    plt.plot([0, 1], [0, 1], '--', c='gray')
    plt.xlabel('假正例率')
    plt.ylabel('真正例率')
    plt.title(f'多分类ROC曲线 (R$^2$={test_r2:.4f})')
    plt.legend()
    plt.tight_layout()
    plt.savefig('2_多分类ROC曲线.png', dpi=300)
    plt.show()
    print("ROC曲线已保存")

    # 特征重要性
    importances = xgb_clf.feature_importances_
    idx_sorted = np.argsort(importances)[::-1]
    sorted_features = [feature_cols[i] for i in idx_sorted]
    sorted_importances = importances[idx_sorted]

    plt.figure(figsize=(10, 6))
    sns.barplot(x=sorted_importances[:10], y=sorted_features[:10], palette='Set2')
    plt.xlabel('特征重要性分数')
    plt.ylabel('特征名称')
    plt.title('Top10特征重要性')
    plt.tight_layout()
    plt.savefig('2_Top10特征重要性.png', dpi=300)
    plt.show()
    print("特征重要性图已保存")

    print("\n模型Top5关键特征：")
    for i in range(min(5, len(sorted_features))):
        print(f"  {i + 1}. {sorted_features[i]}: {sorted_importances[i]:.4f}")

    # 交叉验证
    print("\n正在进行交叉验证...")
    cv_clf = xgb.XGBClassifier(
        objective='multi:softmax',
        num_class=num_classes,
        n_estimators=xgb_clf.best_iteration if hasattr(xgb_clf, 'best_iteration') else xgb_clf.best_iteration_,
        max_depth=2,
        learning_rate=0.15,
        reg_alpha=0.3,
        reg_lambda=0.3,
        subsample=0.6,
        colsample_bytree=0.7,
        random_state=42,
        verbosity=0
    )

    cv_acc_scores = cross_val_score(cv_clf, X_train, y_train, cv=5, scoring='accuracy')
    cv_r2_scores = cross_val_score(cv_clf, X_train, y_train, cv=5, scoring='r2')
    print(f"交叉验证准确率：{cv_acc_scores.mean():.4f} ± {cv_acc_scores.std():.4f}")
    print(f"交叉验证R²值：{cv_r2_scores.mean():.4f} ± {cv_r2_scores.std():.4f}")


# ===================== 5. 主函数（包含特征筛选流程） =====================
def main():
    csv_path = '轴承特征_完整数据.csv'
    try:
        X, y, feature_cols, label_mapping, num_classes, df_valid = load_and_preprocess_data(csv_path)
    except FileNotFoundError:
        print(f"!未找到特征文件：{csv_path}")
        return

    X_train, X_test, y_train, y_test = split_train_test(X, y)

    from sklearn.preprocessing import StandardScaler
    import joblib

    scaler_full = StandardScaler()
    X_train_full_scaled = scaler_full.fit_transform(X_train)
    X_test_full_scaled = scaler_full.transform(X_test)

    joblib.dump(scaler_full, 'scaler_full.pkl')
    print("完整特征标准化器已保存为 'scaler_full.pkl'")

    # 用全部特征训练基础模型
    print("\n===== 第一步：训练基础模型获取特征重要性 =====")
    base_model = train_xgboost_classifier(X_train_full_scaled, y_train, num_classes)

    # 获取特征重要性并筛选Top10特征
    importances = base_model.feature_importances_
    X_train_selected, selected_cols = select_top_features(X_train_full_scaled, feature_cols, importances, top_k=10)
    X_test_selected, _ = select_top_features(X_test_full_scaled, feature_cols, importances, top_k=10)

    # 创建并拟合筛选后的特征标准化器
    scaler_selected = StandardScaler()
    X_train_selected_scaled = scaler_selected.fit_transform(X_train_selected)
    X_test_selected_scaled = scaler_selected.transform(X_test_selected)

    # 保存筛选特征标准化器
    joblib.dump(scaler_selected, 'scaler_selected.pkl')
    print("筛选特征标准化器已保存为 'scaler_selected.pkl'")

    # 用筛选后的特征重新训练模型
    print("\n===== 第二步：使用筛选后的特征重新训练模型 =====")
    xgb_clf = train_xgboost_classifier(X_train_selected_scaled, y_train, num_classes)

    # 保存模型
    joblib.dump(xgb_clf, 'xgb_model.pkl')
    print("XGBoost模型已保存为 'xgb_model.pkl'")

    # 保存筛选后的特征名称
    np.save('selected_features.npy', np.array(selected_cols))
    print("筛选特征名称已保存为 'selected_features.npy'")

    np.save('X_source_selected.npy', X_train_selected_scaled)
    np.save('y_source.npy', y_train)
    print("筛选后的源域特征已保存为 'X_source_selected.npy'")
    print("源域标签已保存为 'y_source.npy'")

    # 评估最终模型
    evaluate_model(xgb_clf, X_train_selected_scaled, X_test_selected_scaled, y_train, y_test,
                   label_mapping, selected_cols, num_classes)

    print("\n故障诊断建模流程全部完成！")

# ===================== 执行入口 =====================
if __name__ == "__main__":
    main()
附录3
介绍：该代码是python语言编写的，为问题三的处理代码。
import numpy as np
import os
import matplotlib.pyplot as plt
from scipy.io import loadmat
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                             f1_score,  classification_report)
from sklearn.manifold import TSNE
from collections import Counter
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from pywt import wavedec
import warnings
import pickle
import scipy.signal
import scipy.stats
from datetime import datetime

# 配置与初始化
warnings.filterwarnings('ignore')
plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"使用设备：{device}")

# 结果保存目录
current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
RESULT_DIR = f"bearing_results_{current_time}"
os.makedirs(RESULT_DIR, exist_ok=True)
print(f"结果保存目录：{os.path.abspath(RESULT_DIR)}")

CONFIG = {
    "source_root": r'C:\Users\zpq\Desktop\E题\源域数据集',
    "target_root": r'C:\Users\zpq\Desktop\E题\目标域数据集',
    "target_unknown_save": {
        "features": "target_unknown_features.npy",
        "preds": os.path.join(RESULT_DIR, "target_unknown_preds.npy"),
        "preds_txt": os.path.join(RESULT_DIR, "target_unknown_labels.txt"),
        "dann_model": os.path.join(RESULT_DIR, "best_dann_model.pth"),
        "tsne_plot": os.path.join(RESULT_DIR, "domain_distribution_tsne.png"),
        "unknown_dist": os.path.join(RESULT_DIR, "unknown_label_distribution.png")
    },
    "save_paths": {
        "source_features": "source_features.npy",
        "source_labels": "source_labels.npy",
        "target_features": "target_features.npy",
        "target_labels": "target_labels.npy",
        "scaler": "scaler.pkl",
        "model": os.path.join(RESULT_DIR, "best_source_model.pth"),
        "metrics": os.path.join(RESULT_DIR, "source_metrics.json"),
        "confusion_matrix": os.path.join(RESULT_DIR, "source_confusion_matrix.png"),
        "test_report": os.path.join(RESULT_DIR, "source_test_report.txt"),
        "train_curve": os.path.join(RESULT_DIR, "source_train_curve.png")
    },
    "fault_map": {'N': 0, 'B': 1, 'IR': 2, 'OR': 3},
    "fault_names": ["正常(N)", "滚动体(B)", "内圈(IR)", "外圈(OR)"],
    "signal_params": {"seg_len": 1024, "fs": 12000, "wavelet": "db4", "level": 3},
    "train": {"lr": 1e-4, "epochs": 50, "batch_size": 32, "test_size": 0.2}
}

# 2.核心工具函数
def convert_numpy_types(obj):
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {k: convert_numpy_types(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(v) for v in obj]
    else:
        return obj


# 3. 信号处理与特征提取
class SignalProcessor:
    @staticmethod
    def wavelet_denoise(signal):
        try:
            coeffs = wavedec(signal, CONFIG["signal_params"]["wavelet"],
                             level=CONFIG["signal_params"]["level"])
            sigma = np.median(np.abs(coeffs[-1])) / 0.6745
            threshold = sigma * np.sqrt(2 * np.log(len(signal)))
            for i in range(1, len(coeffs)):
                coeffs[i] = np.where(np.abs(coeffs[i]) > threshold, coeffs[i], 0)
            return coeffs
        except Exception as e:
            print(f"小波去噪失败：{str(e)}")
            return [signal]

    @staticmethod
    def extract_impact_features(signal, fs):
        window_size = max(int(fs * 0.01), 10)
        kurtosis_vals = []
        for i in range(0, len(signal) - window_size, window_size // 2):
            win_data = signal[i:i + window_size]
            kurtosis_vals.append(scipy.stats.kurtosis(win_data))

        if not kurtosis_vals:
            return 0.0
        threshold = np.percentile(kurtosis_vals, 80)
        impact_indices = np.where(np.array(kurtosis_vals) > threshold)[0]

        if len(impact_indices) < 5:
            return 0.0
        impact_times = impact_indices * (window_size // 2) / fs
        interval_std = np.std(np.diff(impact_times))
        return 1 / (interval_std + 1e-8)

    @staticmethod
    def extract_features(signal_segment):
        fs = CONFIG["signal_params"]["fs"]
        seg_len = CONFIG["signal_params"]["seg_len"]

        # 基础时域特征
        mean_val = np.mean(signal_segment)
        std_val = np.std(signal_segment)
        rms_val = np.sqrt(np.mean(signal_segment ** 2))
        peak_val = np.max(np.abs(signal_segment))
        crest_factor = peak_val / (rms_val + 1e-8)
        impulse_factor = peak_val / (np.mean(np.abs(signal_segment)) + 1e-8)
        kurtosis_val = np.mean((signal_segment - mean_val) ** 4) / (std_val ** 4 + 1e-8)

        # 傅里叶变换基础
        fft_vals = np.fft.fft(signal_segment)
        fft_abs = np.abs(fft_vals)[:seg_len // 2]
        freqs = np.fft.fftfreq(seg_len, 1 / fs)[:seg_len // 2]
        total_energy = np.sum(fft_abs) + 1e-8
        abs_signal = np.abs(signal_segment)

        # 外圈故障专属特征
        or_high_band = (freqs >= 2200) & (freqs <= 2800)
        or_peak_threshold = 0.3 * np.max(fft_abs[or_high_band]) if np.sum(or_high_band) > 0 else 0
        or_peak_count = np.sum(fft_abs[or_high_band] > or_peak_threshold)

        ORF = 1400  # 外圈特征频率
        orf_band = (freqs >= ORF - 30) & (freqs <= ORF + 30)
        orf_energy_ratio = np.sum(fft_abs[orf_band]) / total_energy

        analytic_signal = scipy.signal.hilbert(abs_signal)
        envelope = np.abs(analytic_signal)
        envelope_fft = np.fft.fft(envelope)
        envelope_fft_abs = np.abs(envelope_fft)[:seg_len // 2]
        envelope_freqs = np.fft.fftfreq(seg_len, 1 / fs)[:seg_len // 2]
        valid_mask = envelope_freqs > 10
        envelope_peak_threshold = 0.2 * np.max(envelope_fft_abs[valid_mask]) if np.sum(valid_mask) > 0 else 0
        or_envelope_harmonic = np.sum(envelope_fft_abs[valid_mask] > envelope_peak_threshold)

        # 滚动体故障特征
        BSF = 1560  # 滚动体特征频率
        bsf_band = (freqs >= 1500) & (freqs <= 1620)
        bsf_energy_ratio = np.sum(fft_abs[bsf_band]) / total_energy

        bsf_peak_concentration = 0.0
        if np.sum(bsf_band) > 0:
            bsf_peak = np.max(fft_abs[bsf_band])
            bsf_peak_count = np.sum(fft_abs[bsf_band] > 0.5 * bsf_peak)
            bsf_peak_concentration = bsf_peak_count / (np.sum(bsf_band) + 1e-8)

        ftf_modulation_coeff = 0.0
        FTF = 4.5  # 旋转频率
        if np.sum(bsf_band) > 0:
            bsf_peak_idx = np.argmax(fft_abs[bsf_band])
            bsf_peak_freq = freqs[bsf_band][bsf_peak_idx]
            bsf_peak_energy = fft_abs[bsf_band][bsf_peak_idx] + 1e-8

            modulated_energy = 0
            for delta in [-2, -1, 1, 2]:
                mod_freq = bsf_peak_freq + delta * FTF
                if 1500 <= mod_freq <= 1620:
                    mod_mask = (freqs >= mod_freq - 2) & (freqs <= mod_freq + 2)
                    modulated_energy += np.max(fft_abs[mod_mask]) if np.sum(mod_mask) > 0 else 0
            ftf_modulation_coeff = modulated_energy / bsf_peak_energy

        impact_feature = SignalProcessor.extract_impact_features(signal_segment, fs)

        # 内圈故障专属特征
        IRF = 1620  # 内圈特征频率
        irf_band = (freqs >= IRF - 30) & (freqs <= IRF + 30)
        irf_energy_ratio = np.sum(fft_abs[irf_band]) / total_energy

        ir_harmonic_interval_std = 0.0
        if np.sum(valid_mask) > 0:
            envelope_peaks = envelope_freqs[valid_mask][envelope_fft_abs[valid_mask] > envelope_peak_threshold]
            if len(envelope_peaks) > 2:
                ir_harmonic_interval_std = np.std(np.diff(envelope_peaks))

        # 其他基础特征
        pulse_index = np.max(abs_signal) / (np.sum(abs_signal > 0.5 * np.max(abs_signal)) + 1e-8)
        denoised_coeffs = SignalProcessor.wavelet_denoise(signal_segment)
        detail_coeff = denoised_coeffs[-1]
        detail_peak_diff = np.max(detail_coeff) - np.min(detail_coeff)

        envelope_peak_freq = envelope_freqs[valid_mask][np.argmax(envelope_fft_abs[valid_mask])] if np.sum(
            valid_mask) > 0 else 0

        freq_centroid = np.sum(freqs * fft_abs) / total_energy
        spec_power = fft_abs / total_energy
        spec_entropy = -np.sum(spec_power * np.log2(spec_power + 1e-10))
        peak_indices = np.argsort(fft_abs)[-4:][::-1] if len(fft_abs) >= 4 else np.argsort(fft_abs)[::-1]
        peak_freqs = [freqs[i] for i in peak_indices] + [0] * (4 - len(peak_indices))

        wavelet_energy = [np.sum(coeff ** 2) for coeff in denoised_coeffs]
        total_wavelet_energy = np.sum(wavelet_energy)
        energy_ratio = wavelet_energy[-1] / (total_wavelet_energy + 1e-8)

        # 组合所有特征并清洗
        features = np.array([
            mean_val, std_val, rms_val, peak_val, crest_factor, impulse_factor, kurtosis_val,
            or_peak_count, orf_energy_ratio, or_envelope_harmonic,
            pulse_index, detail_peak_diff, envelope_peak_freq,
            freq_centroid, spec_entropy, peak_freqs[0], peak_freqs[1], energy_ratio,
            bsf_energy_ratio, bsf_peak_concentration, ftf_modulation_coeff, impact_feature,
            irf_energy_ratio, ir_harmonic_interval_std,
            peak_freqs[2], peak_freqs[3], total_wavelet_energy, np.sum(or_high_band)
        ], dtype=np.float32)

        features = np.clip(features, -1000, 1000)
        features[np.isnan(features)] = 0
        features[np.isinf(features)] = 1000
        return features


#  4. 数据加载
def load_data(data_root, is_source=True, force_reload=False, load_labels=True):
    # 1. 确定特征和标签保存路径
    if is_source:
        feat_path = CONFIG["save_paths"]["source_features"]
        label_path = CONFIG["save_paths"]["source_labels"]
    else:
        if load_labels:
            feat_path = CONFIG["save_paths"]["target_features"]
            label_path = CONFIG["save_paths"]["target_labels"]
        else:
            feat_path = CONFIG["target_unknown_save"]["features"]
            label_path = CONFIG["target_unknown_save"]["features"].replace("features", "labels")

    # 2. 加载已保存的特征
    if not force_reload and os.path.exists(feat_path):
        print(f"加载已保存的{'源域' if is_source else '目标域'}特征...")
        features = np.load(feat_path)
        if load_labels:
            if os.path.exists(label_path):
                labels = np.load(label_path)
                labels = labels.flatten() if labels.ndim > 1 else labels
            else:
                raise FileNotFoundError(f"标签文件不存在：{label_path}")
        else:
            labels = np.full(len(features), -1)
        print(f"加载完成：特征形状{features.shape} | 标签形状{labels.shape}")
        return features, labels

    # 3. 重新加载原始数据并提取特征
    all_features = []
    all_labels = []
    if not os.path.exists(data_root):
        raise FileNotFoundError(f"数据路径不存在：{data_root}")

    print(f"\n{'=' * 50}\n开始加载{'源域' if is_source else '目标域'}数据（路径：{data_root}）")
    for root, _, files in os.walk(data_root):
        for fname in files:
            if not fname.endswith('.mat'):
                continue
            fpath = os.path.join(root, fname)

            # 标签解析：区分有标签/无标签
            if load_labels:
                # 有标签数据
                fault_type = None
                if 'N' in fname:
                    fault_type = 'N'
                elif 'B' in fname and 'IR' not in fname and 'OR' not in fname:
                    fault_type = 'B'
                elif 'IR' in fname:
                    fault_type = 'IR'
                elif 'OR' in fname:
                    fault_type = 'OR'
                else:
                    print(f"跳过未知标签文件：{fname}")
                    continue
                label = CONFIG["fault_map"][fault_type]
            else:
                # 无标签数据
                label = -1

            # 加载信号并提取特征
            try:
                mat_data = loadmat(fpath)
                signal_keys = [k for k in mat_data.keys() if
                               not k.startswith('__') and isinstance(mat_data[k], np.ndarray)]
                if not signal_keys:
                    print(f"无有效信号，跳过：{fname}")
                    continue
                signal = mat_data[signal_keys[0]].flatten()
                seg_len = CONFIG["signal_params"]["seg_len"]
                if len(signal) < seg_len:
                    print(f"信号过短（长度{len(signal)}），跳过：{fname}")
                    continue

                # 信号分割
                num_segs = len(signal) // seg_len
                for i in range(num_segs):
                    seg = signal[i * seg_len: (i + 1) * seg_len]
                    feat = SignalProcessor.extract_features(seg)  # 复用28维特征提取
                    all_features.append(feat)
                    all_labels.append(label)

                # 仅源域滚动体样本增强
                if is_source and load_labels and fault_type == 'B':
                    for i in range(0, num_segs, 2):  # 每2个样本增强1个
                        seg = signal[i * seg_len: (i + 1) * seg_len]
                        noisy_seg = seg + 0.05 * np.random.randn(len(seg)) * np.std(seg)  # 加噪增强
                        feat = SignalProcessor.extract_features(noisy_seg)
                        all_features.append(feat)
                        all_labels.append(label)
                    print(f"已处理（含增强）：{fname}（{num_segs + num_segs // 2}段，标签：{fault_type}）")
                else:
                    print(f"已处理：{fname}（{num_segs}段，标签：{'无标签' if label == -1 else label}）")

            except Exception as e:
                print(f"处理{fname}出错：{str(e)}，跳过")
                continue

    # 数据校验与保存
    if len(all_features) < 10:
        raise ValueError(f"有效样本不足（{len(all_features)}个），请检查数据！")
    features = np.array(all_features, dtype=np.float32)
    labels = np.array(all_labels, dtype=np.int32)
    labels = labels.flatten() if labels.ndim > 1 else labels

    # 保存特征和标签（无标签数据仅保存特征）
    np.save(feat_path, features)
    if load_labels:
        np.save(label_path, labels)

    print(f"\n数据加载完成：{len(features)}个样本，每个样本{features.shape[1]}维特征")
    if load_labels:
        print(f"标签分布：{Counter(labels)}")
    return features, labels


#  5. 数据集定义
class BearingDataset(Dataset):
    def __init__(self, features, labels=None, is_train=True, is_target=False):
        assert features.ndim == 2, f"特征必须是2D数组，实际是{features.ndim}D"
        if not is_train and not is_target and labels is None:
            raise ValueError("测试集初始化时必须传入labels！")
        self.features = features
        self.labels = labels
        self.is_train = is_train
        self.is_target = is_target

        # 标准化处理
        if is_train:
            self.scaler = RobustScaler()
            self.features = self.scaler.fit_transform(features)
            with open(CONFIG["save_paths"]["scaler"], 'wb') as f:
                pickle.dump(self.scaler, f)
        else:
            if not os.path.exists(CONFIG["save_paths"]["scaler"]):
                raise FileNotFoundError("标准化器文件不存在，请先训练训练集！")
            with open(CONFIG["save_paths"]["scaler"], 'rb') as f:
                self.scaler = pickle.load(f)
            self.features = self.scaler.transform(features)

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feat = torch.tensor(self.features[idx], dtype=torch.float32)
        if self.labels is not None:
            label_scalar = self.labels[idx].item() if isinstance(self.labels[idx], np.ndarray) else self.labels[idx]
            lbl = torch.tensor(label_scalar, dtype=torch.long)
        else:
            lbl = torch.tensor(-1, dtype=torch.long)  # 无标签数据
        return feat, lbl

# 6. 源域分类模型
class CategoryAwareClassifier(nn.Module):
    def __init__(self, input_dim, num_classes=4):
        super().__init__()
        self.input_dim = input_dim

        # 特征注意力层
        self.attention = nn.Sequential(
            nn.Linear(input_dim, input_dim // 2),
            nn.Tanh(),
            nn.Linear(input_dim // 2, input_dim),
            nn.Softmax(dim=1)
        )

        # 类别专属特征权重
        self.class_feat_weights = nn.Parameter(torch.tensor([
            1.0,  # 正常(N)
            1.0,  # 滚动体(B)
            1.2,  # 内圈(IR)
            1.1  # 外圈(OR)
        ], dtype=torch.float32).unsqueeze(1).repeat(1, input_dim))

        # 分类网络
        self.classifier = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.BatchNorm1d(64),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )

    def forward(self, x):
        if x.dim() == 3:
            x = x.squeeze(1)
        assert x.shape[1] == self.input_dim, f"输入维度应为{self.input_dim}，实际是{x.shape[1]}"

        # 基础注意力
        att_weights = self.attention(x)
        x_att = x * att_weights

        # 初步分类
        logits = self.classifier(x_att)

        # 类别感知特征加权
        class_weights = torch.softmax(logits, dim=1) @ self.class_feat_weights
        x_final = x_att * class_weights

        # 最终分类
        return self.classifier(x_final)


#  7. DANN迁移学习
class GradientReversalLayer(torch.autograd.Function):

    @staticmethod
    def forward(ctx, x, alpha=1.0):
        ctx.alpha = alpha
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output * (-ctx.alpha), None


class DANNBearingModel(nn.Module):

    def __init__(self, input_dim=28, num_classes=4):
        super().__init__()
        self.input_dim = input_dim

        # 1. 特征提取器
        self.feature_extractor = nn.Sequential(
            # 复用原有注意力层
            nn.Linear(input_dim, input_dim // 2),
            nn.Tanh(),
            nn.Linear(input_dim // 2, input_dim),
            nn.Softmax(dim=1),
            nn.Linear(input_dim, 128),  # 源域分类器第一层
            nn.ReLU(),
            nn.BatchNorm1d(128),
            nn.Dropout(0.3),
            nn.Linear(128, 64),  # 特征输出：64维领域不变特征
            nn.ReLU()
        )
        # 2. 标签分类器
        self.label_classifier = nn.Sequential(
            nn.BatchNorm1d(64),
            nn.Dropout(0.2),
            nn.Linear(64, num_classes)
        )

        # 3. 领域鉴别器
        self.domain_classifier = nn.Sequential(
            nn.BatchNorm1d(64),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()  # 输出0=源域，1=目标域
        )

        # 梯度反转层实例
        self.grl = GradientReversalLayer()

    def forward(self, x, alpha=1.0):
        # 特征提取：输出64维领域不变特征
        features = self.feature_extractor(x)

        # 标签预测
        label_logits = self.label_classifier(features)

        # 领域预测（通过梯度反转层，与特征提取器对抗）
        reversed_features = self.grl.apply(features, alpha)  # 应用梯度反转
        domain_logits = self.domain_classifier(reversed_features)

        return label_logits, domain_logits

#  8. 模型评估工具函数
def evaluate_model(model, dataloader, device, save_path=None, is_dann=False):
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for feats, lbls in dataloader:
            feats, lbls = feats.to(device), lbls.to(device)
            if is_dann:
                label_logits, _ = model(feats)
                preds = torch.argmax(label_logits, dim=1)
            else:
                preds = torch.argmax(model(feats), dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(lbls.cpu().numpy())

    # 计算整体指标
    metrics = {
        "accuracy": accuracy_score(all_labels, all_preds),
        "precision": precision_score(all_labels, all_preds, average='weighted'),
        "recall": recall_score(all_labels, all_preds, average='weighted'),
        "f1": f1_score(all_labels, all_preds, average='weighted')
    }

    # 计算每个类别的指标
    class_metrics = {}
    for i, name in enumerate(CONFIG["fault_names"]):
        y_true_binary = (np.array(all_labels) == i).astype(int)
        y_pred_binary = (np.array(all_preds) == i).astype(int)

        if np.sum(y_true_binary) > 0:
            class_metrics[name] = {
                "precision": precision_score(y_true_binary, y_pred_binary, average='binary', pos_label=1),
                "recall": recall_score(y_true_binary, y_pred_binary, average='binary', pos_label=1),
                "f1": f1_score(y_true_binary, y_pred_binary, average='binary', pos_label=1),
                "support": np.sum(y_true_binary)
            }
        else:
            class_metrics[name] = {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "support": 0
            }

    # 保存分类报告
    if save_path:
        report = classification_report(
            all_labels, all_preds,
            target_names=CONFIG["fault_names"],
            digits=4
        )
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"分类报告已保存至：{save_path}")

    return metrics, class_metrics, all_preds, all_labels


#  9. DANN训练与目标域预测函数
def train_dann(source_loader, target_unlabeled_loader, input_dim=28):
    # 初始化DANN模型
    model = DANNBearingModel(input_dim=input_dim).to(device)

    # 损失函数：标签分类损失(源域)+领域对抗损失（源域+目标域）
    class_criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.2, 1.1, 0.8], dtype=torch.float32).to(device))
    domain_criterion = nn.BCELoss()  # 二分类损失（源域/目标域）

    # 优化器：同时优化特征提取器、标签分类器、领域鉴别器
    optimizer = optim.Adam(model.parameters(), lr=CONFIG["train"]["lr"] * 0.8)  # 学习率略低
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)

    best_domain_loss = float('inf')
    print(f"\n{'=' * 50}\n开始DANN迁移学习（{CONFIG['train']['epochs']}轮）")

    for epoch in range(CONFIG["train"]["epochs"]):
        model.train()
        total_class_loss = 0.0
        total_domain_loss = 0.0

        # 迭代源域和目标域数据
        source_iter = iter(source_loader)
        target_iter = iter(target_unlabeled_loader)
        iter_num = min(len(source_loader), len(target_unlabeled_loader))

        for _ in range(iter_num):
            # 1. 加载源域有标签数据
            src_feats, src_labels = next(source_iter)
            src_feats, src_labels = src_feats.to(device), src_labels.to(device)
            src_domain = torch.zeros(len(src_feats), 1).to(device)

            # 2. 加载目标域无标签数据
            tgt_feats, _ = next(target_iter)
            tgt_feats = tgt_feats.to(device)
            tgt_domain = torch.ones(len(tgt_feats), 1).to(device)

            # 3. 对抗系数alpha：随迭代线性增大，平衡分类与对抗
            alpha = min(1.0, 0.1 + epoch / CONFIG["train"]["epochs"] * 0.9)

            # 4. 前向传播
            all_feats = torch.cat([src_feats, tgt_feats], dim=0)
            label_logits, domain_logits = model(all_feats, alpha=alpha)

            # 5. 计算损失
            class_loss = class_criterion(label_logits[:len(src_feats)], src_labels)
            domain_loss = domain_criterion(domain_logits, torch.cat([src_domain, tgt_domain], dim=0))
            total_loss = class_loss + 0.5 * domain_loss

            # 6. 反向传播与优化
            optimizer.zero_grad()
            total_loss.backward()
            optimizer.step()

            # 累计损失
            total_class_loss += class_loss.item()
            total_domain_loss += domain_loss.item()

        # 学习率衰减
        scheduler.step()

        # 每5轮打印训练状态
        if (epoch + 1) % 5 == 0:
            avg_class_loss = total_class_loss / iter_num
            avg_domain_loss = total_domain_loss / iter_num
            print(f"Epoch {epoch + 1}/{CONFIG['train']['epochs']} | Alpha: {alpha:.2f}")
            print(f"分类损失：{avg_class_loss:.4f} | 领域损失：{avg_domain_loss:.4f}")

            # 保存领域损失最小的模型
            if avg_domain_loss < best_domain_loss:
                best_domain_loss = avg_domain_loss
                torch.save(model.state_dict(), CONFIG["target_unknown_save"]["dann_model"])
                print(f"保存最佳DANN模型（领域损失：{best_domain_loss:.4f}）")

    # 加载最佳DANN模型
    model.load_state_dict(torch.load(CONFIG["target_unknown_save"]["dann_model"]))
    return model


def predict_unknown_target(model):
    # 1.加载目标域未知数据
    unknown_feats, _ = load_data(
        data_root=CONFIG["target_root"],
        is_source=False,
        force_reload=False,
        load_labels=False
    )

    # 2.创建数据集和数据加载器
    unknown_dataset = BearingDataset(
        features=unknown_feats,
        labels=None,
        is_train=False,
        is_target = True
    )
    unknown_loader = DataLoader(
        unknown_dataset,
        batch_size=CONFIG["train"]["batch_size"],
        shuffle=False
    )

    # 3.预测标签
    model.eval()
    all_preds = []
    with torch.no_grad():
        for feats, _ in unknown_loader:
            feats = feats.to(device)
            label_logits, _ = model(feats)
            preds = torch.argmax(label_logits, dim=1)
            all_preds.extend(preds.cpu().numpy())

    # 4.保存预测结果
    np.save(CONFIG["target_unknown_save"]["preds"], all_preds)

    # 保存TXT格式，包含故障类型名称
    with open(CONFIG["target_unknown_save"]["preds_txt"], 'w', encoding='utf-8') as f:
        f.write("目标域未知数据预测标签（索引: 预测类别）\n")
        f.write("=" * 50 + "\n")
        for idx, pred in enumerate(all_preds):
            f.write(f"样本 {idx:05d}: {CONFIG['fault_names'][pred]}\n")
    print(f"\n目标域未知数据标签已保存至：{CONFIG['target_unknown_save']['preds_txt']}")

    # 5. 绘制未知数据标签分布
    plt.figure(figsize=(8, 6))
    pred_counts = Counter(all_preds)
    counts = [pred_counts.get(i, 0) for i in range(4)]
    plt.bar(CONFIG["fault_names"], counts, color=['green', 'red', 'blue', 'orange'])
    plt.title('目标域未知数据预测标签分布', fontsize=14)
    plt.xlabel('故障类型', fontsize=12)
    plt.ylabel('样本数量', fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(CONFIG["target_unknown_save"]["unknown_dist"], dpi=300)
    plt.close()
    print(f"未知数据标签分布已保存至：{CONFIG['target_unknown_save']['unknown_dist']}")

    return all_preds


def visualize_domain_transfer(model, source_loader, target_loader):
    model.eval()
    all_features = []
    all_domains = []

    # 收集源域特征
    with torch.no_grad():
        for feats, _ in source_loader:
            feats = feats.to(device)
            features = model.feature_extractor(feats)
            all_features.append(features.cpu().numpy())
            all_domains.extend([0] * len(feats))

    # 收集目标域特征
    with torch.no_grad():
        for feats, _ in target_loader:
            feats = feats.to(device)
            features = model.feature_extractor(feats)
            all_features.append(features.cpu().numpy())
            all_domains.extend([1] * len(feats))

    # TSNE降维
    print("\n正在进行TSNE降维可视化领域分布...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    features_2d = tsne.fit_transform(np.vstack(all_features))

    # 绘图
    plt.figure(figsize=(10, 8))
    plt.scatter(
        features_2d[np.array(all_domains) == 0, 0],
        features_2d[np.array(all_domains) == 0, 1],
        c='blue', label='源域', alpha=0.6, s=50
    )
    plt.scatter(
        features_2d[np.array(all_domains) == 1, 0],
        features_2d[np.array(all_domains) == 1, 1],
        c='red', label='目标域', alpha=0.6, s=50
    )
    plt.title('DANN迁移前后领域特征分布（TSNE可视化）', fontsize=14)
    plt.xlabel('TSNE维度1', fontsize=12)
    plt.ylabel('TSNE维度2', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(CONFIG["target_unknown_save"]["tsne_plot"], dpi=300)
    plt.close()
    print(f"领域分布可视化已保存至：{CONFIG['target_unknown_save']['tsne_plot']}")

# 10.主流程控制函数
def main():
    # 1：训练源域分类模型
    print(f"{'=' * 50}\n阶段1/3：训练源域分类模型")
    source_features, source_labels = load_data(
        CONFIG["source_root"],
        is_source=True,
        force_reload=False
    )
    input_dim = source_features.shape[1]

    # 划分训练集和测试集
    X_train, X_test, y_train, y_test = train_test_split(
        source_features, source_labels,
        test_size=CONFIG["train"]["test_size"],
        random_state=42,
        stratify=source_labels
    )

    # 创建数据集和DataLoader
    train_dataset = BearingDataset(X_train, y_train, is_train=True)
    test_dataset = BearingDataset(X_test, y_test, is_train=False)
    train_loader = DataLoader(
        train_dataset,
        batch_size=CONFIG["train"]["batch_size"],
        shuffle=True,
        drop_last=True
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=CONFIG["train"]["batch_size"],
        shuffle=False
    )

    # 初始化并训练源域模型
    source_model = CategoryAwareClassifier(input_dim=input_dim).to(device)
    class_counts = Counter(y_train)
    weights = [
        len(y_train) / (len(class_counts) * class_counts[i]) if i != 1 and i != 2 and i != 3 else
        len(y_train) / (class_counts[i] * 1.2) if i == 1 else
        len(y_train) / (class_counts[i] * 1.1) if i == 2 else
        len(y_train) / (class_counts[i] * 0.8)
        for i in range(4)
    ]
    class_weights = torch.tensor(weights, dtype=torch.float32).to(device)
    criterion = nn.CrossEntropyLoss(weight=class_weights)
    optimizer = optim.Adam(source_model.parameters(), lr=CONFIG["train"]["lr"])
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.8)

    best_acc = 0.0
    train_history = []
    for epoch in range(CONFIG["train"]["epochs"]):
        source_model.train()
        train_loss = 0.0
        for feats, lbls in train_loader:
            feats, lbls = feats.to(device), lbls.to(device)
            optimizer.zero_grad()
            outputs = source_model(feats)
            loss = criterion(outputs, lbls)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        scheduler.step()
        if (epoch + 1) % 5 == 0:
            metrics, class_metrics, _, _ = evaluate_model(source_model, test_loader, device)
            avg_train_loss = train_loss / len(train_loader)
            train_history.append({
                "epoch": epoch + 1,
                "train_loss": avg_train_loss,
                "val_accuracy": metrics["accuracy"],
                "val_f1": metrics["f1"]
            })
            print(f"\nEpoch {epoch + 1}/{CONFIG['train']['epochs']}")
            print(f"学习率：{scheduler.get_last_lr()[0]:.6f}")
            print(f"训练损失：{avg_train_loss:.4f}")
            print(f"验证准确率：{metrics['accuracy']:.4f} | 验证F1值：{metrics['f1']:.4f}")
            if metrics["accuracy"] > best_acc:
                best_acc = metrics["accuracy"]
                torch.save(source_model.state_dict(), CONFIG["save_paths"]["model"])
                print(f"最佳模型更新：当前最佳准确率 {best_acc:.4f}")

    # 2：训练DANN迁移模型
    print(f"\n{'=' * 50}\n阶段2/3：训练DANN迁移模型")
    target_feats, _ = load_data(
        data_root=CONFIG["target_root"],
        is_source=False,
        force_reload=False,
        load_labels=False
    )
    # 创建目标域无标签数据集
    target_dataset = BearingDataset(
        features=target_feats,
        labels=None,
        is_train=False,
        is_target = True
    )
    target_loader = DataLoader(
        target_dataset,
        batch_size=CONFIG["train"]["batch_size"],
        shuffle=True
    )
    # 训练DANN模型
    dann_model = train_dann(
        source_loader=train_loader,
        target_unlabeled_loader=target_loader,
        input_dim=input_dim
    )
    # 可视化领域迁移效果
    visualize_domain_transfer(dann_model, test_loader, target_loader)

    # 3：预测目标域未知数据标签
    print(f"\n{'=' * 50}\n阶段3/3：预测目标域未知数据标签")
    unknown_preds = predict_unknown_target(dann_model)
    print(f"目标域未知数据标签预测完成，共{len(unknown_preds)}个样本")
    print(f"标签分布：{Counter([CONFIG['fault_names'][p] for p in unknown_preds])}")
    print(f"\n所有结果已保存至：{os.path.abspath(RESULT_DIR)}")

# 主函数调用
if __name__ == "__main__":
    main()
